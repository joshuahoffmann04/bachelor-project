sources:
  pruefungsordnung:
    type: pdf
    path: data/raw/Pruefungsordnung_BSc_Inf_2024.pdf
    enabled: true
    priority: high
  modulhandbuch:
    type: web
    url: https://www.mathematik.uni-marburg.de/modulhandbuch/20252/BSc_Mathematik/index.html
    update_frequency: weekly
    cache_enabled: true
    enabled: true
    priority: high
  veranstaltungskalender:
    type: web
    url: https://www.mathematik.uni-marburg.de/studium/preview/index.html?lang=de
    update_frequency: daily
    cache_enabled: true
    enabled: true
    priority: medium
scraping:
  user_agent: 'UniMarburgRAGBot/1.0 (Educational Research; Contact: research@uni-marburg.de)'
  rate_limit: 1
  timeout: 30
  cache_expiry: 86400
  respect_robots_txt: true
  max_retries: 3
  retry_delay: 2
  playwright:
    headless: true
    browser: chromium
    wait_for_selector: body
    screenshot_on_error: true
pdf_processing:
  parsers:
  - pymupdf
  - pdfplumber
  - tabula
  table_extraction:
    enabled: true
    preserve_formatting: true
    merge_cells: true
  ocr:
    enabled: false
    language: deu
chunking:
  strategy: hybrid
  semantic:
    enabled: true
    split_by:
    - paragraph
    - section
    - heading
    min_chunk_size: 100
    max_chunk_size: 256
  sliding_window:
    chunk_size: 256
    overlap: 51
  special_handling:
    tables:
      split: false
      max_size: 1024
    lists:
      keep_together: true
    legal_references:
      preserve: true
embeddings:
  model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
  batch_size: 32
  normalize: true
  device: cpu
retrieval:
  hybrid:
    enabled: true
    dense_weight: 0.7
    sparse_weight: 0.3
    fusion_method: rrf
  dense:
    top_k: 10
    similarity_threshold: 0.0
    index_type: flatl2
  sparse:
    top_k: 10
    k1: 1.5
    b: 0.75
  final_top_k: 5
  reranking:
    enabled: false
    model: cross-encoder/ms-marco-MiniLM-L-6-v2
llm:
  provider: openai
  openai:
    api_key: ${OPENAI_API_KEY}
    model: gpt-4-turbo-preview
    temperature: 0.1
    max_tokens: 1000
    top_p: 0.95
    frequency_penalty: 0.0
    presence_penalty: 0.0
  claude:
    api_key: ${ANTHROPIC_API_KEY}
    model: claude-3-opus-20240229
    temperature: 0.1
    max_tokens: 1000
  vllm:
    base_url: http://localhost:8000
    model: meta-llama/Llama-2-70b-chat-hf
    temperature: 0.1
    max_tokens: 1000
  generation:
    stream: false
    timeout: 60
    max_retries: 3
  safety:
    enable_content_filter: true
    max_output_length: 2000
prompts:
  system_prompt_file: config/prompts/system_prompt.txt
  user_prompt_template_file: config/prompts/user_prompt_template.txt
  citation:
    required: true
    format: 'Quelle: {source_doc}, Seite {page}, {section}'
  abstaining:
    threshold: 0.6
    message: Ich kann diese Frage nicht sicher auf Basis der vorliegenden Dokumente
      beantworten.
evaluation:
  ragas:
    enabled: true
    metrics:
    - context_relevance
    - faithfulness
    - answer_relevance
    - answer_correctness
  custom_metrics:
  - ects_accuracy
  - reference_quality
  - abstaining_rate
  - hallucination_rate
  test_dataset:
    path: data/test_sets/
    auto_generate: true
    num_questions: 100
    categories:
    - ects_lookup
    - prerequisites
    - deadlines
    - exam_formats
    - out_of_scope
logging:
  level: INFO
  format: json
  file:
    enabled: true
    path: logs/rag_system.log
    rotation: daily
    retention: 30
  privacy:
    anonymize_queries: true
    no_personal_data: true
    aggregate_only: true
  metrics:
    query_latency: true
    retrieval_scores: true
    generation_time: true
    cache_hits: true
    error_rates: true
caching:
  enabled: true
  embeddings:
    enabled: true
    path: data/processed/embedding_cache.pkl
  scraping:
    enabled: true
    path: data/scraped/
    expiry: 86400
  query:
    enabled: true
    ttl: 3600
    max_size: 1000
api:
  host: 0.0.0.0
  port: 8000
  workers: 4
  reload: false
  cors:
    enabled: true
    origins:
    - http://localhost:3000
    - http://localhost:8000
  rate_limit:
    enabled: true
    requests_per_minute: 60
experiments:
  enabled: true
  variants:
    chunk_sizes:
    - 256
    - 512
    - 1024
    top_k_values:
    - 3
    - 5
    - 10
    embedding_models:
    - sentence-transformers/paraphrase-multilingual-mpnet-base-v2
    - sentence-transformers/all-MiniLM-L6-v2
    fusion_weights:
    - dense: 0.7
      sparse: 0.3
    - dense: 0.5
      sparse: 0.5
    - dense: 0.8
      sparse: 0.2
performance:
  max_workers: 4
  batch_size: 32
  max_memory_gb: 8
  gpu:
    enabled: false
    device_id: 0
version:
  system_version: 1.0.0
  config_version: 1.0.0
  data_version: '2024.1'
